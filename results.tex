This chapter presents a detailed performance analysis of our two subsumption-based tabling engines.
We divided this chapter into four sections. The first section describes the
set of tabled benchmark programs used. The second
section evaluates the engine supporting traditional call subsumption that was implemented by integrating the
Time Stamped Tries algorithms and data structures from XSB Prolog into Yap Prolog. This includes analyzing the memory gains
of call subsumption by measuring the size of subgoal and answer tries and comparing them to variant-based tabling.
In the third section, we evaluate the retroactive-based tabling engine for programs that do not benefit
from the new mechanisms and for programs that can take advantage of this new evaluation method. Finally,
in the fourth section, we evaluate the STST table space overhead in a potentially not so good scenario,
where the operations of loading and storing answers are more expensive than usual.

\section{Benchmark Programs}

In order to assess the performance of our tabling engines we used various programs and data sets.
We next briefly describe the programs used (see Appendix~\ref{app:code} for more details).

\begin{description}
   
   \item[path:] This program computes the reachability between two nodes in a graph.
   Connections between two nodes are represented by \texttt{edge/2} facts.
   We used the following graph configurations in our tests: \textbf{tree}, a
   binary tree; \textbf{chain}, a chain of nodes; \textbf{cycle}, a chain of nodes, where the
   last node connects with the first one; \textbf{pyramid}, a pyramid-like configuration;
   and \textbf{grid}, where nodes are connected in a grid-like fashion.
   For the \texttt{path/2} predicate itself, we used 6 different versions: \textbf{left\_first},
   \textbf{left\_last}, \textbf{right\_first}, \textbf{right\_last},
   \textbf{double\_first}, and \textbf{double\_last}.
    
   \item[samegen:] The \texttt{samegen/2} predicate solves the same generation problem.
   For this program, we used the configurations described above for \textbf{path}.
   
   \item[genome:] This program computes the set of nodes that are reachable by nodes 1 and 2 in a graph.
   This is an interesting problem, since it creates lots of subsumed consumers when using
   call subsumption. We also used the same configurations described above for \textbf{path}.
   To compute reachability this program uses the \textbf{left\_last} version.
   
   \item[reach:] The \texttt{reach/2} predicate computes the reachability in a relation graph for a set of
   model specifications. The benchmark is actually a set of programs originally taken from the
   XMC project~\cite{RamakrishnanCR-00}\footnote{http://www.cs.sunysb.edu/\char`\~lmc/}, which is a model checker implemented
   atop the XSB system. We used two variants of the \texttt{reach/2} predicate,
   \textbf{reach\_first} and \textbf{reach\_last}.
   The following relation graphs where used:
   
      \begin{description}
         
         \item[iproto:] \emph{i-protocol} specification defined for a correct version with a huge window size.
         
         \item[leader:] \emph{leader election} specification defined for 5 processes.
         
         \item[sieve:] \emph{sieve} specification defined for 5 processes and 4 overflow prime numbers.

      \end{description}
      
   \item[flora:] This program was generated by an object-oriented knowledge base language and application 
               development environment known as FLORA-2 \cite{Yang-00}\footnote{http://flora.sourceforge.net}.
               
   \item[fib:] This program uses a \texttt{fib/2} predicate to compute the Fibonacci number of a given
   parameter which allows to benchmark the pruning of one subsumed subgoal.
   
   \item[big:] This program also uses the \texttt{fib/2} predicate, but instead of one, multiple subsumed subgoals
   are called and pruned. As a parameter, we can input the number of subsumed subgoals to call and prune.
   
\end{description}

Again, note that the relevant parts of the Prolog code for these programs are presented in Appendix~\ref{app:code}.

The environment for our experiments was an Intel Core(TM) 2 Quad 2.66 GHz with 4 GBytes of
memory and running the Linux kernel 2.6.31 with Yap Prolog 6.0.3 and XSB Prolog 3.2.
The scheduling strategy used by default was batched scheduling.

\section{Traditional Call Subsumption with TST}

In this section, we first evaluate the performance of YapTab against the SLG-WAM,
by comparing the gains obtained by using call subsumption instead of variant checks.
In the second part of this section, we measure the impact in terms of space by using
call subsumption. For this, we compared the number of answer and subgoal trie nodes
created when using variant checks and when using subsumptive checks.

\subsection{Performance Evaluation}

In order to compare the YapTab tabling engine with the SLG-WAM we used the following benchmarks:

\begin{itemize}
   \item The \texttt{path/2} program with all combinations of versions and data sets and the query goal `\texttt{path(X,Y)}'.
   
   \item The \texttt{samegen/2} program with all data sets and the query goal `\texttt{samegen(X,Y)}'.
   
   \item The \texttt{genome/1} program with all data sets and the query goal `\texttt{genome(X)}'.
   
   \item The two versions of the \texttt{reach/2} program with the following queries for each relation graphs:

   \begin{itemize}
      \item iproto: `\texttt{reach(iproto\_0(\_,\_,end),Y)}'.
      \item leader: `\texttt{reach(systemLeader\_0(5,end),Y)}'.
      \item sieve: `\texttt{reach(sieve\_0(5,4,27,end),Y)}'.
   \end{itemize}

\end{itemize}

For each benchmark, we used variant-based tabling and then subsumption-based tabling.
Next, we computed the execution time and compared the speedups obtained ($T_{variant} / T_{subsumptive}$) for
each engine. The times presented next are the average of 3 runs. Given that YapTab's implementation
is largely based on XSB's code to implement the subsumption mechanisms,
we expect the speedups to be very similar. Some potential differences between them will arise because
of particular characteristics, namely: the way they implement the tabling algorithms, the WAM engine itself,
the compiled trie code, and the handling of answer templates.

Table~\ref{tbl:results_overview} summarizes the average speedups obtained for each program,
while Tables~\ref{tbl:result_detail_path} and \ref{tbl:result_detail_others}
show the full details, with times and speedups for YapTab and SLG-WAM.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \textbf{SLG-WAM} & \textbf{YapTab} \\
    & \textit{\small{Average Speedup}} & \textit{\small{Average Speedup}} \\
   \hline
   \hline
left\_first & 0.78 & \textbf{1.02} \\
left\_last & 0.77  & \textbf{0.96} \\
right\_first & \textbf{1.01} & \textbf{1.01} \\
right\_last & 0.94 & \textbf{1.07} \\
double\_first & 1.37 & \textbf{1.48} \\
double\_last & 1.31 & \textbf{1.40} \\
samegen & \textbf{339.76} & 1.03 \\
genome & 559.54 & \textbf{648.51} \\
reach\_first  & \textbf{0.96} & 0.94 \\
reach\_last  & \textbf{0.97} & 0.90 \\
\hline
\hline
\end{tabular}
\caption{Average speedups for call subsumption in SLG-WAM and YapTab.}
\label{tbl:results_overview}
\end{table}

The first thing we note is that,  YapTab has a better speedup than SLG-WAM in 6 benchmarks, while in 3 of
them SLG-WAM wins. In most of the benchmarks, the speedups for the two engines are very similar, which proves
that our integration efforts were  largely successful. However, for the \textbf{samegen} benchmark, the
speedups are not very similar at all, because the SLG-WAM engine has an average speedup of 339.76
and YapTab only 1.03. This happens because the performance of the variant-based version of SLG-WAM
performs very poorly against YapTab, which explains such big differences (see Table~\ref{tbl:result_detail_others}
for details).

The programs \textbf{left\_first} and \textbf{left\_last} do not generate any subsumed consumer,
therefore they are good benchmarks to assess the overhead of using the subsumption mechanisms. For YapTab,
the overhead is minimal with an average speedup of 0.96 for the \textbf{left\_last} program. Surprisingly, for the
\textbf{left\_first} program the speedup is 1.02, that is, the subsumptive-based engine performed better
than the variant-based engine. The SLG-WAM behavior for these two programs is clearly worst, with an average
speedup of 0.78 and 0.77 for \textbf{left\_first} and \textbf{left\_last}, respectively.

On the other hand, the programs \textbf{right\_first} and \textbf{right\_last} do generate subsumed consumers,
as many as the number of \texttt{edge/2} facts. Notably, only YapTab
achieves an average speedup bigger than 1 for both programs. These programs show poor speedups because simple facts
are faster to evaluate than to use the time stamped trie to collect relevant answers.
In particular, the binary tree graph configuration with the \textbf{right\_first} and \textbf{right\_last} programs
in YapTab has a very poor speedup of 0.92 and 0.33, respectively, which is clearly influencing the average speedups
(see Table~\ref{tbl:result_detail_path}).
In the \textbf{right\_first} benchmark, the time stamp index is created right at
the beginning of the program, when the time stamped trie is still empty, and maintained thereafter, but,
in the case of the \textbf{right\_last} program, the indices are only created when the recursive
clause is executed, and at that time the trie already contains a considerable amount of answers.
We thus modified the subsumptive
engine to create the time stamp index from the beginning, and the \textbf{right\_last} program had considerable
better results. Therefore, we argue that the lazy creation of the time stamp index can affect considerably the
execution time, for programs where consumers appear when the answer trie already contains lots of answers.
The operation of creating the time stamp index
and sorting each trie node at each trie level can be very costly when a large number of nodes exist.
By experimentation we found that it seems more efficient to maintain the index as the answer trie is being expanded.

For the \textbf{double\_first} and \textbf{double\_last} we have attained speedups between 1.31 and 1.48
for both YapTab and SLG-WAM. These benchmarks are more computationally expensive given that they create more
dependencies in variant-based tabling. However, in subsumption-based tabling, the number of dependencies is reduced 
and thus less program clauses are explored.

The \textbf{genome} program shows the best speedup results, with an impressive average speedup of 
648.51 for YapTab. In this program, the subgoal \texttt{path(2,X)} and \texttt{path(1,X)} are called
very early in the evaluation which makes all further subgoals calls to \texttt{path/2} that are subsumed by these
two goals, as consumers.

For the model checking programs, the results were not so good with close speedups for YapTab and SLG-WAM.

\input{tables_path}
\input{tables_model_checking}

\subsection{Memory Usage}

In this section we measure the size of the table space for the variant and the subsumptive engine.
As a metric, we use the number of allocated answer trie nodes across all answer tries
and the number of allocated subgoal trie nodes across all tabled predicates.
For this, we used the following benchmarks:

\begin{itemize}
   \item The programs \textbf{left\_first}, \textbf{right\_first} and \textbf{double\_first} with
   all the data sets and the query `\texttt{path(X,Y)}'. Note that there is no need to use
   the \textbf{last} versions of these programs, because they will result in the same number of answer trie nodes.
   
   \item The \textbf{samegen} program with all data sets and the query goal `\texttt{samegen(X,Y)}'.
   
   \item The \textbf{genome} program with all data sets and the query goal `\texttt{genome(X)}'.
   
   \item The \textbf{flora} program with the query goal `\texttt{\'\_\$\_\$\_flora\_isa\_rhs\'(\_,direct)}'.
\end{itemize}

Table~\ref{tbl:results_detail_space_sub} presents the results we obtained for the number of created
answer trie nodes. From the table, we can see that,
for the \texttt{left\_first} program, where no subsumed subgoals are called, the number of answer trie nodes created is
exactly same when compared to variant-based tabling. Please notice that, while the number of trie nodes is
the same, the size of the nodes in subsumption-based tabling make
the time stamped answer trie 16.6\% more costly in terms of memory usage, because they
have the extra \texttt{timestamp} field.

For the \textbf{right\_first} and \textbf{double\_first} programs, the number of answer trie nodes
is reduced in half. Using variant-based tabling, these programs generate a large number of subgoals,
that in subsumption-based tabling are consumers of the first called subgoal, thus creating more
answer tries for each one of these subgoals, and thus more answer trie nodes globally.
For the \textbf{genome} benchmark, we need only to store from 66\% to 75\% of the nodes used for
variant-based tabling.

\input{tables_sub_space}

The \texttt{samegen/2} program presents a curious behavior for the \texttt{cycle} and \texttt{chain}
data sets. In these cases, the program generates an answer that subsumes all the other answers and
every other subgoal call is subsumed by the top subgoal, resulting in only 3 created answer trie nodes,
the root node, and the two nodes for the two terms of the general solution. Compared to tabling with
variant checks, this result in an impressive space saving.

Table~\ref{tbl:results_detail_space_sub_subgoal} present the results for the number of
allocated subgoal trie nodes for the benchmarks shown in Table~\ref{tbl:results_detail_space_sub}.

\input{tables_sub_space_subgoal}

The programs \textbf{left\_first},
\textbf{right\_first} and \textbf{double\_first} all create the same number of subgoal trie nodes
for all data sets. The \textbf{left\_first} program allocates a generator and one variant consumer,
therefore only 3 subgoal trie nodes are needed.
When using subsumptive tabling, both the
programs \textbf{right\_first} and \textbf{double\_first} create subsumed consumers that are
represented on the subgoal trie, but for variant-based tabling, these consumers are actually
generators that are also represented in the subgoal trie. Because the two engines create the same
number of subgoal trie nodes, we argue that using subsumptive-based tabling on these programs
does not decrease the number of unique called subgoals.

For the \textbf{genome} benchmark, we observe that there is an impressive reduction in stored
subgoal trie nodes. Because the subgoal \texttt{path(2,X)} is called and completed early in the
program, subsequent consumers of this subgoal are not inserted on the subgoal trie, but they
directly execute compiled trie code. Therefore, for subsumptive-based tabling only three subgoals
are represented on the subgoal tries: \texttt{genome(X)}, \texttt{path(1,X)}, \texttt{path(2,X)}
and a fourth subgoal for the predicate \texttt{path/2} that is instantiated in the two arguments.

For the \textbf{flora} benchmark, we observe a considerable reduction in both subgoal and answer trie nodes
when using subsumptive-based tabling. The \textbf{flora} benchmark shows that it is possible to
apply call subsumption in complex programs in order to reduce the table space and the number of
tabled subgoals and thus maximize the sharing of answers.

\section{Retroactive Call Subsumption with STST}

In this section we evaluate our retroactive-based tabling engine implemented on
top of YapTab. First, we start by assessing the overhead of using the new mechanisms that
support the RCS engine. In the second part of this section, we evaluate
the RCS engine with programs where specific subgoals are called before general subgoals, in
order to assess the advantages of the new mechanism.

\subsection{Support Mechanisms Overhead}

To measure the overhead of our RCS engine, we executed programs where RCS is never applied, i.e., where
general subgoals are always called before subsumed subgoals. Therefore we can estimate the impact in the
execution time of the RCS support mechanisms, namely: building the subgoal dependency tree, the STST table space,
and searching for running subsumed subgoals.
We used the following benchmarks:

\begin{itemize}
   \item The \texttt{path/2} program with all combinations of versions and data sets and the query goal `\texttt{path(X,Y)}'.
   
   \item The \texttt{samegen/2} program with all data sets and the query goal `\texttt{samegen(X,Y)}'.
   
   \item The two versions of the \texttt{reach/2} program with the following queries for each relation graphs:

   \begin{itemize}
      \item iproto: `\texttt{reach(iproto\_0(\_,\_,end),Y)}'.
      \item leader: `\texttt{reach(systemLeader\_0(5,end),Y)}'.
      \item sieve: `\texttt{reach(sieve\_0(5,4,27,end),Y)}'.
   \end{itemize}

\end{itemize}

Notice that compared to traditional call subsumption, RCS should have
relatively identical execution times with a minimal overhead, because RCS also reuses answers when a subsumed subgoal is
called after a subsuming subgoal.

For each table, we present the average execution time of 3 runs of the RCS engine and its relative time
to the other YapTab engines by computing the value $T_{RCS} / T_{engine}$.
If the value is greater than 1.0 then RCS performs worse, otherwise RCS performs better.

Table~\ref{tbl:overhead_overview} summarizes the average overheads obtained for each program,
while Tables~\ref{tbl:overhead_detail_tst} and \ref{tbl:overhead_detail_model} show the detailed
results, with execution time in milliseconds and overheads, for YapTab.
By analyzing the results we can see that, on average, YapTab with RCS performs worse
than YapTab with subsumptive-based tabling in most cases, and only the
programs \textbf{right\_first} and \textbf{right\_last} show better results,
while the \textbf{left\_first} and \textbf{left\_last} programs have very comparable execution times.
In theory, these benchmarks should not run faster, but cache effects and other
conditions could affect positively the execution time of these programs.

Regarding the total average results, RCS is about 5\% slower than traditional call subsumption,
which shows that RCS adds a very small overhead when executing programs that do not benefit from the
new evaluation model. Compared to variant-based tabling, RCS is about 10\% slower for these benchmarks,
and it is only in the \texttt{right\_first} program that RCS performs better.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \multicolumn{2}{c}{\textbf{YapTab}} \\
    & \textit{\small{Retro / Var}} & \textit{\small{Retro / Sub}} \\
   \hline
   \hline
   left\_first & 1.06 & 1.01 \\
   left\_last &  1.07  & 1.03 \\
   right\_first & \textbf{0.97} & \textbf{0.95} \\
   right\_last & 1.25 & \textbf{0.94} \\
   double\_first & 1.01 & 1.16 \\
   double\_last & 1.04 & 1.16 \\
   samegen & 1.19 & 1.14 \\
   reach\_first  &  1.11  & 1.04 \\
   reach\_last  &  1.17  & 1.04 \\
\hline
\hline
\textit{Total Average} &  1.10 &  1.05 \\
\hline
\hline
\end{tabular}
\caption{Average overhead for programs not taking advantage of RCS.}
\label{tbl:overhead_overview}
\end{table}

\input{tables_retro_overhead_tst}
\input{tables_retro_overhead_model}

\subsection{Performance Evaluation}

In this section, we present experimental results using retroactive-based tabling on programs that
can benefit from it, i.e., programs where general subgoals are called after more specific subgoals.
The programs we used for these experiments are the following:

\begin{itemize}
   \item The programs \textbf{left\_first}, \textbf{left\_last}, \textbf{double\_first} and \textbf{double\_last}
   with all data sets and the query goal `\texttt{path(X,1)}'. Note that by calling the subgoal \texttt{path(X,1)},
   the subgoal \texttt{path(X,Y)} is called in the continuation, which prunes the first subgoal.
   
   \item The \texttt{genome/1} program with all data sets and the query goal `\texttt{genome(X)}'.
   
   \item The two versions of the \texttt{reach/2} program with the following queries for each relation graphs:

   \begin{itemize}
      \item iproto: `\texttt{reach(iproto\_0(\_,\_,end),imain\_7\_0(A,~B,~C,~D,~E))}'.
      \item leader: `\texttt{reach(systemLeader\_0(5,end),~par(D,~E,~A,~B))}'.
      \item sieve: `\texttt{reach(sieve\_0(5,4,27,end),~par(A,~B,~C,~D))}'.
   \end{itemize}
   
   \item The \textbf{flora} program with the query goal `\texttt{\'\_\$\_\$\_flora\_isa\_rhs\'(\_,direct)}'.
   
   \item The \textbf{fib} program with the parameters 30, 32, 35, and 36 and the query goal `\texttt{a(X),~p(Y,~Z)}'.
   
   \item The \textbf{big} program with the parameters (number of subgoals to prune) 5, 10, and 20 and the
   query goal `\texttt{a(X)}'.
\end{itemize}

We next present the results with the average execution time of the retroactive-based
engine and compare it with both variant and subsumptive YapTab tabling engines using the formula $T_{engine} / T_{RCS}$.
All execution times are an average of 3 runs. Table~\ref{tbl:results_gain_overview} summarizes the average values of
each program, while the detailed results are presented in Tables~\ref{tbl:results_detail_gain_tst} and
\ref{tbl:results_detail_gain_model}.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \multicolumn{2}{c}{\textbf{YapTab}} \\
    & \textit{\small{Var / Retro}} & \textit{\small{Sub / Retro}} \\
   \hline
   \hline
left\_first & 0.89 & 0.95 \\
left\_last & 0.88  & 0.90 \\
double\_first & 1.07 & \textbf{1.09} \\
double\_last & 1.05 & \textbf{1.10} \\
genome & 450.33 & 0.74 \\
reach\_first  & 2.54 & \textbf{1.76} \\
reach\_last  & 3.22 & \textbf{1.87} \\
flora & 3.17 & \textbf{1.17} \\
fib & 1.95 & \textbf{2.02} \\
big & 13.26 & \textbf{13.66} \\
\hline
\hline
\end{tabular}
\caption{Average speedups for RCS in YapTab.}
\label{tbl:results_gain_overview}
\end{table}

For the \texttt{path/2} we should distinguish between the versions where the recursive clause is the first
clause (\textbf{left\_first} and \textbf{double\_first}) and the versions where the recursive clause is the second
clause (\textbf{left\_last} and \textbf{double\_last}). In the \textbf{first} versions, the specific subgoal
generates answers before reaching the general subgoal, while in the \textbf{last} versions, the general subgoal
is reached first. Therefore, the \textbf{first} versions should, in principle, obtain better results than the \textbf{last}
versions, because they waste less time executing the subsumed subgoal. The results for the \textbf{left} versions of the
\texttt{path/2} program confirm this, with speedups of 0.95 and 0.90 for the \textbf{left\_first} and
\textbf{left\_last} versions, respectively. Surprisingly, the \textbf{double} versions show the opposite behavior.


\input{tables_gain_tst}

Another important observation we can make from these results is that RCS not always shows performance gains, when
programs takes advantage of it. In our experiments, only the programs \textbf{left\_first}, \textbf{left\_last} and
\textbf{genome} show, in average, worse performance.


We argue that the speedup of using RCS is highly dependent on the nature of the program. The \textbf{left} version
of the \texttt{path/2} program, for example, does not show improvements because what we gain from pruning the
execution of simple \texttt{edge/2} facts does not pay the cost of using the STST to retrieve relevant answers
for the subsumed subgoal and the cost of pruning the computation itself. In other words, using subsumptive-based
tabling for this program seems advantageous because the cost of executing more predicate clauses is less than
maintaining the time stamp index.

Even if the \texttt{path/2} programs, in average, do not show considerable improvements, for some data sets these
programs can obtain good speedups, for example, the binary tree configuration can reach a speedup of 1.61.


\input{tables_gain_model}

Regarding the model checking programs, \textbf{reach\_left} and \textbf{reach\_last}, as they are much more complex
than the \textbf{path} data sets, they show larger improvements. In particular, for the \textbf{reach\_last} program
with the \textbf{leader} model, we obtain a good speedup of 2.16. The \textbf{flora} program also shows an interesting
improvement with a speedup of 1.17 over traditional call subsumption. These complex benchmarks show that RCS has the
potential to speedup the execution on this type of programs.

For the \textbf{fib} program, RCS shows a considerable average speedup of 2.02 over call subsumption and an identical
speedup of 1.95 against variant-based tabling. Because the execution time of the pruned subsumed subgoal is more or less
equal to that of the subsuming subgoal the speedup is maintained for the different parameters (see 
Table~\ref{tbl:results_detail_gain_model}). For the \textbf{big} program, where multiple subsumed subgoals are
pruned, we can see that as the number of pruned subsumed subgoals increases proportionally, the speedup also increases,
reaching a maximum of 21.98. An interesting observation for the \textbf{big} program is that the execution time for
retroactive-tabling stays constant (around 580 milliseconds), when we increase the number of subsumed subgoals to prune.


\section{Single Time Stamped Trie}

In the STST table space organization, the answers for all the subgoals
of a predicate are stored in a single answer trie. While advantageous, all arguments of
the answers must be stored in the trie. In this section, we experiment with programs
where this property is stressed to measure the overhead in terms of space and time, when
the load operation is more expensive and the store operation needs to insert more terms in
the trie than what is needed to complete the computation.

For this, we used the \texttt{path/2} program. We transformed, both the program and data sets,
to use a functor term in each argument, instead of simple integers. For example, an
\texttt{edge(3,4)} fact is transformed into \texttt{edge(f(3),f(4))}. The updated version
of the \textbf{left\_first} program is illustrated in Figure~\ref{fig:converted_path}.
We experimented the query goal `\texttt{path(f(X),f(Y))}' with different graph sizes with
three different versions of the \texttt{path/2} program.

\begin{figure}[ht]
\begin{Verbatim}
path(f(X),f(Y)) :- path(f(X),f(Z)), edge(f(Z),f(Y)).
path(f(X),f(Y)) :- edge(f(X),f(Y)).
\end{Verbatim}
\caption{Transformed \texttt{path/2} predicate to use functor term arguments.}
\label{fig:converted_path}
\end{figure}

\subsection{Execution Time}

The average results are presented in Table~\ref{tbl:results_average_stst} and compare the STST to
tabling with variant and subsumptive checks. Table~\ref{tbl:results_detail_stst} present the detailed
results concerning execution time for each benchmark. Note that each execution time is the average
of 3 runs. From these results, we see that, on average, the transformed \texttt{path/2} program has an
overhead of 28\%, which is considerable when compared to the overhead of 5\% found early on this chapter.

The consumption of answers by consumers and the insertion of new answers by generators into the table
space are the primary causes for the overhead in these benchmarks. The programs with the
worst overhead are \textbf{double\_first} and \textbf{double\_last}, with 48\% and 49\% of overhead
against traditional call subsumption. These programs also create the higher number of consumers,
both variant consumers and subsumed consumers than any other benchmark in these experiments.
The \textbf{right\_first} and \textbf{right\_last} only create subsumed consumers, and they have an overhead
of 14\% and 12\%, respectively, which are the lowest overhead values. In the programs \textbf{left\_first} and
\textbf{left\_last}, only one variant consumer is allocated, however they perform worse than the \textbf{right} versions.

We thus argue that the number of consumer nodes can greatly reduce the
applicability and performance of the STST table space organization when the operation of loading an answer
from the trie is more expensive. While this situation seems disadvantageous, execution time can
be reduced if another subgoal call appears (for example \texttt{path(X,Y)}) where its possible to
reuse the answers from the table before executing the predicate clauses, thus providing an
elegant solution to the problem of incomplete tables.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \multicolumn{2}{c}{\textbf{YapTab}} \\
    & \textit{\small{Retro / Var}} & \textit{\small{Retro / Sub}} \\
   \hline
   \hline
   left\_first & 1.22 & 1.20 \\
   left\_last & 1.27 & 1.27 \\
   right\_first & 1.18 & 1.14 \\
   right\_last & 1.15 & 1.12 \\
double\_first & 2.56 & 1.48 \\
double\_last & 2.50 & 1.49 \\
\hline
\hline
\textit{Total Average} & 1.65 & 1.28 \\
\hline
\hline
\end{tabular}
\caption{Average overheads for the query goal `\texttt{path(f(X),f(Y))}' in YapTab.}
\label{tbl:results_average_stst}
\end{table}

\input{tables_stst_overhead}

\subsection{Memory Usage}

We executed the previous benchmarks and measured the number of answer trie nodes for each program.
Table~\ref{tbl:results_average_stst_space} presents the average results of RCS against variant and
subsumptive-based engines in YapTab. The detailed results are presented in Table~\ref{tbl:results_detail_stst_space}.
In this table, we show the number of answer trie nodes allocated for the RCS execution and then the relative number
of trie nodes for the variant and subsumptive-based executions.
The programs \textbf{left}, \textbf{right} and \textbf{double} are the left, right and double versions
of the \texttt{path/2} program. Note that we could use a single \texttt{path/2} program version, but
we used different versions to distinguish between different graph sizes.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \multicolumn{2}{c}{\textbf{YapTab}} \\
    & \textit{\small{Var / Retro}} & \textit{\small{Sub / Retro}} \\
   \hline
   \hline
   left & 0.99258 & 0.99258 \\
   right & 1.95122 & 0.97906 \\
   double & 2.72892 & 0.90149 \\
\hline
\hline
\textit{Total Average} & 1.89091 &  0.95771 \\
\hline
\hline
\end{tabular}
\caption{Average number of stored answer trie nodes for the query goal `\texttt{path(f(X),f(Y))}' in YapTab.}
\label{tbl:results_average_stst_space}
\end{table}

From these results we can see that the STST table space is 89\% more efficient
than the variant table space. In particular, for the \textbf{double} program, these differences are higher
because in the variant engine there are more generator subgoal calls and thus more answer tries are created.

\input{tables_stst_space}

When comparing RCS to the subsumptive-based engine, the subsumptive-based engine only stores 4.2\% less trie nodes
than the RCS engine, even if the \texttt{f/1} functor terms need to be stored. This is easily understandable because
the first \texttt{f/1} functor term is only represented once, at the top of the STST, and then there is one
second \texttt{f/1} functor
for each source node in the graph, therefore, the total number of functors stored is insignificant when
compared to the total number of terms stored in the trie. Also note that, for the \textbf{double} benchmarks,
the data sets used are small if compared to the data sets used for the other benchmarks, but the space overhead
is more significant (18\% in the worst case). We thus argue that the cost of
the extra space needed to store terms in the STST is less significant as more terms are stored in the trie.
