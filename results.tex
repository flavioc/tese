This chapter presents a detailed performance analysis of the two subsumption-based tabling engines
that we have developed. We divided this chapter into four sections. The first section describes the
set of tabled benchmark programs that we have used to assess the performance of execution. The second
section evaluates the engine with traditional call subsumption that was implemented by integrating the
Time-Stamped Tries algorithms and data structures from XSB into Yap. Next, we analyze the memory gains
of call subsumption by measuring the size of the answer tries and comparing it to variant-based tabling.
In the third section, we evaluate the retroactive-based tabling engine for programs that do not benefit
from the new mechanisms and
for programs that take advantage of the new evaluation method. Finally, we evaluate the STST table space
overhead in a bad scenario, where the operations of loading and storing stores are more expensive than
usual.

\section{Benchmark Programs}

In order to assess the performance of our tabling engines we used various programs and data sets.
The following list describes the programs used:

\begin{description}
   
   \item[path:] The \texttt{path/2} program computes the reachability between two nodes in a graph.
   Connections between two nodes are represented by \texttt{edge/2} facts.
   We used the following graph configurations in our tests: \textbf{tree}, graph is a
   binary tree; \textbf{chain}, graph is chain of nodes; \textbf{cycle}, graph is a chain, but
   last node on the chain connects with the first node; \textbf{pyramid}, a pyramid configuration;
   and \textbf{grid}, where nodes are connected in a grid-like fashion.
   For the \texttt{path/2} program itself, we used 6 different versions: \texttt{left\_first},
   \texttt{left\_last}, \texttt{right\_first}, \texttt{right\_last},
   \texttt{double\_first}, and \texttt{double\_last}.
    
   \item[samegen:] The \texttt{samegen/2} predicate solves the same generation problem.
   For this test, we used the same data sets of the \texttt{path/2} program.
   
   \item[genome:] This program computes the set of nodes that are reached by node 1 and node 2.
   This is an interesting problem, since it creates lots of subsumed consumers when using
   call subsumption. We also used the same data sets of the \texttt{path/2} program.
   To compute reachability this program uses the \texttt{left\_last} program.
   
   \item[reach:] The \texttt{reach/2} computes the reachability in a relation graph for a set of
   model specifications. The benchmark is actually a set of programs originally taken from the
   XMC project~\cite{system-xmc,RamakrishnanCR-00}, which is a model checker implemented
   atop the XSB system. We used two variants of the \texttt{reach/2} predicate,
   \texttt{reach\_first} and \texttt{reach\_last}.
   The following relation graphs are used:
   
      \begin{description}
         
         \item[sieve:] \emph{sieve} specification defined for 5 processes and 4 overflow prime numbers.
         
         \item[leader:] \emph{leader election} specification defined for 5 processes.
         
         \item[iproto:] \emph{i-protocol} specification defined for a correct version with a huge window size.

      \end{description}
      
   \item[flora:] This program was generated by an object-oriented knowledge base language and application 
               development environment known as FLORA-2 \cite{Yang-00} \footnote{http://flora.sourceforge.net}.
               
   \item[fib:] This program uses the \texttt{fib/2} predicate in order to benchmark the pruning of one subsumed subgoal.
   The program has the Fibonacci number to compute as a parameter.
   
   \item[big:] This program also uses the \texttt{fib/2} predicate, but instead of one, multiple subsumed subgoals
   are called and pruned. As a parameter, we can input the number of subsumed subgoals to call.
   
\end{description}

Note that parts of the code for these programs is presented in Appendix~\ref{app:code}.

The scheduling strategy used by default is batched scheduling.
The environment for our experiments was an Intel Core(TM) 2 Quad 2.66 GHz with 4 GBytes of
memory and running the Linux kernel 2.6.31 with YapTab 6.0.3 and XSB Prolog 3.2.

\section{Performance Evaluation for YapTab\_TST}

In this section, we first evaluate the performance of YapTab with call subsumption against the SLG-WAM,
by comparing the obtained speedup that are gained by using call subsumption instead of variant checks.
In the second part of this section, we measure the impact of using call subsumption in terms of space.
For this, we compared the number of answer trie nodes created using variant checks and with subsumptive
checks.

\subsection{Comparison with the SLG-WAM}

In order to compare the YapTab\_TST tabling engine with the SLG-WAM we used the following programs:

\begin{itemize}
   \item The \texttt{path/2} program with the query `\texttt{?-~path(X,Y).}' with all the combinations of
   versions and data sets.
   \item The \texttt{genome/1} program with different data sets. The query used was `\texttt{?-~genome(X).}'.
   
   \item The query `\texttt{?-~samegen(X,Y).}' in the \texttt{samegen/2} program. All \texttt{path/2} data
   sets were used.
   
   \item The two versions of the \texttt{reach/2} program with the following queries for the relation graphs:

   \begin{itemize}
      \item sieve: `\texttt{?-~reach(sieve\_0(5,4,27,end),Y).}'.
      \item leader: `\texttt{?-~reach(systemLeader\_0(5,end),Y).}'.
      \item iproto: `\texttt{?-~reach(iproto\_0(\_,\_,end),Y).}'.
   \end{itemize}

\end{itemize}

We compared the performance of the SLG-WAM and YapTab with traditional call subsumption.
For each benchmark, we used variant-based tabling and then subsumption-based tabling.
Next, we compared the values of each engine and calculated the speedup ($T_{variant} / T_{subsumptive}$) for
each engine. Given that YapTab\_TST uses a great deal of code from XSB to implement the subsumption mechanisms,
we expect the speedups to be very similar. Some potential differences between them will arise because
of certain characteristics, namely: the way they implement the tabling algorithms, the WAM engine itself,
the compiled trie code, and the handling of answer templates.

Tables~\ref{tbl:result_path_detail1}, \ref{tbl:result_path_detail2} and \ref{tbl:result_model_detail}
show the detailed performance tests, with times and speedups for YapTab and SLG-WAM for each program.
Table~\ref{tbl:results_overview} presents the average speedups for each program taken from the
detailed benchmarks.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \textbf{SLG-WAM} & \textbf{YapTab} \\
    & \textbf{\textit{\small{Speedup}}} & \textbf{\textit{\small{Speedup}}} \\
   \hline
   \hline
double\_first & 1.25 & \textbf{1.46} \\
double\_last & 1.34 & \textbf{1.35} \\
genome & \textbf{596.17} & 595.55 \\
left\_first & 0.76 & \textbf{0.95} \\
left\_last & 0.76  & \textbf{1.00} \\
right\_first & \textbf{1.03} & 0.92 \\
right\_last & \textbf{1.07} & 0.98 \\
samegen & \textbf{292.66} & 0.93 \\
reach\_first  & \textbf{0.95} & 0.89 \\
reach\_last  & \textbf{0.98} & 0.95 \\
\hline
\hline
\end{tabular}
\caption{Benchmarks for call subsumption in SLG-WAM and YapTab.}
\label{tbl:results_overview}
\end{table}

The first thing we note is that, in 4 benchmarks
YapTab has a better speedup than SLG-WAM, while in 6 of them SLG-WAM wins. While SLG-WAM wins here,
the speedups for the two engines are very similar, which proves that our integration
efforts of the Time-Stamped Tries approach to subsumption-based tabling in YapTab was largely successful.
There is one benchmark were the speedups are not very similar at all, which is the \texttt{samegen} benchmark.
The SLG-WAM engine has an average speedup of 292.66 and YapTab only 0.93 because the performance
of the variant-based version of SLG-WAM performs very poorly against YapTab, which explains such big differences.

The programs \texttt{left\_first} and \texttt{left\_last} do not generate any subsumed consumer,
therefore they are good benchmarks to assess the overhead of using subsumption mechanisms. For YapTab,
the overhead is minimal with a speedup of 0.95 for the \texttt{left\_first} program. Surprisingly, for the
\texttt{left\_last} program the speedup is equal to 1, that is, both variant-based and subsumptive-based
engines have the same execution time. SLG-WAM has more overhead, with 0.76 for both programs.

The programs \texttt{right\_first} and \texttt{right\_last} do generate subsumed consumers,
as many as \texttt{edge/2} facts. Notably, only the SLG-WAM with the \texttt{right\_first} program
achieves a speedup bigger than 1. Why there are no speedups for these programs is that simple facts
are faster to evaluate than to use the time stamped trie to collect relevant answers.
In YapTab, the binary tree graph configuration with the \texttt{right\_last} program has a very poor speedup
of 0.31 when compared to a speedup of 0.77 when used with the \texttt{right\_first} program.
In the \texttt{right\_first} benchmark, the time stamped index is created right at
the beginning of the program when the time stamped trie is still empty and maintained thereafter, but,
in the case of the \texttt{right\_last} program, the indices are only created when the recursive
clause is executed, when the trie already contains a considerable amount of answers. We modified the subsumptive
engine to create the time stamp index from the beginning, and the \texttt{right\_last} program had considerable
better results. Therefore, we argue that the lazy creation of the time stamped index can affect considerably the
execution time. For programs where subsumption happens, the benchmarks may perform worse if a consumer appears
when the answer trie already contains lots of answers. The operation of creating the time stamped index
and sorting each trie node at each trie level is a very costly operation when a high number of nodes exist.
Sometimes, it is more efficient to maintaing the index as the answer trie is being expanded.

For the \texttt{double\_first} and \texttt{double\_last} we have attained speedups between 1.25 and 1.46
for each configuration. These benchmarks are more computationally expensive given that they create more
dependencies. These dependencies diminish in subsumption-based tabling because only new consumers are created
and code is not executed.

The \texttt{genome} program attained very good speedup results, with
595.55 for YapTab. In this program, the subgoal \texttt{path(2,X)} and \texttt{path(1,X)} are called
very early in the evaluation and further subgoals calls to \texttt{path/2} that are subsumed by these
goals are consumers.

For the model checking programs, the results were not so good for subsumption-based evaluation with
identical speedups for YapTab and SLG-WAM.

\input{tables_model_checking}
\input{tables_path}

\subsection{Space Analysis}

In this section we measure the size of the table space, for the variant and the subsumptive engine.
As a metric, we use the number of allocated answer trie nodes across all answer tries.
For this, we used the following benchmarks:

\begin{itemize}
   \item The programs \texttt{left\_first}, \texttt{right\_first} and \texttt{double\_first} with the query `\texttt{?-~path(X,Y).}'. Note that there is no need to use
   the \texttt{last} versions of these programs, because they will result in the same number of answer trie nodes.
   
   \item The \texttt{genome/1} program.
   
   \item The \texttt{samegen/2} program.
   
   \item The \texttt{flora} benchmark.
\end{itemize}

Table~\ref{tbl:results_detail_space_sub} presents the results we obtained. From the table, we see that,
for some programs where no subsumed subgoals are called, the number of answer trie nodes created is
exactly same when compared to variant-based tabling. For example, in the programs \texttt{genome/1} and
\texttt{left\_first} we can observe this behavior. Please notice that, while the number of trie nodes is
the same, the size of the nodes in subsumption-based tabling (they have a \textbf{timestamp} field) make
the time stamped answer trie more costly in terms of memory used.

In the programs \texttt{right\_first} and \texttt{double\_first}, the number of answer trie nodes
is reduced in half. Using variant-based tabling, these programs generate a large number of subgoals,
that in subsumption-based tabling are consumers of the first called subgoal, thus creating more
answer tries for each one of these subgoals, and thus more answer trie nodes. As one would expect,
using subsumption-based tabling can reduce substantially the table space.

The \texttt{samegen/2} program presents a curious behavior for the \texttt{cycle} and \texttt{chain}
data sets. In these cases, the program generates an answer that subsumes all the other answers and
every other subgoal call is subsumed by the top subgoal, resulting in only 3 answer trie nodes created,
the root node, and the two nodes for the two terms of the general solution. Compared to tabling with
variant checks, this result in very good space savings.
The \texttt{flora} benchmark also shows a reduced table space size, which shows that subsumption-based
tabling can be successfully applied to complex programs, resulting in a largely reduced
table space.

\input{tables_sub_space}

\section{Performance Evaluation for RCS}

In this section we evaluate our retroactive-based tabling engine that we implemented on
top of YapTab. First, we start by assessing the overhead of using the new mechanisms that
support the RCS engine, namely: building the subgoal dependency tree, the STST table space,
and searching for running subsumed subgoals. In the second part of this section, we evaluate
the RCS engine with programs where specific subgoals are called before general subgoals, in
order to assess the advantages of the new mechanism.

\subsection{RCS Overhead}

To measure the overhead, we executed programs where general subgoals are always called before
subsumed subgoals (or not at all), therefore we can estimate the impact in the execution time
because the pruning techniques of RCS are not employed.
We used the following benchmarks:

\begin{itemize}
   \item Every combination of versions and data sets for the \texttt{path/2} program with the query `\texttt{?-~path(X,Y).}'.
   
   \item The query `\texttt{?-~samegen(X,Y).}' in the \texttt{samegen/2} program. All \texttt{path/2} data
   sets were used.
   
   \item The two versions of the \texttt{reach/2} program with the following queries for the relation graphs:

   \begin{itemize}
      \item sieve: `\texttt{?-~reach(sieve\_0(5,4,27,end),Y).}'.
      \item leader: `\texttt{?-~reach(systemLeader\_0(5,end),Y).}'.
      \item iproto: `\texttt{?-~reach(iproto\_0(\_,\_,end),Y).}'.
   \end{itemize}
\end{itemize}

We timed the execution of the benchmarks for the RCS engine and then the execution time
for the subsumptive and variant-based engines of both YapTab and SLG-WAM. In the next tables,
we present the execution time of the RCS engine in milliseconds and the relative time
of the other engines by computing the value $T_{engine} / T_{RCS}$.
If the value is lesser than 1.0 then RCS performs worse, otherwise if the value is greater than
1.0 then RCS performs better. At the end of each table, we present the average values for
each engine.

Tables~\ref{tbl:overhead_detail_tst1}, \ref{tbl:overhead_detail_tst2} and \ref{tbl:overhead_detail_model}
show the detailed results for the programs we use to measure the overhead.
Table~\ref{tbl:overhead_overview} shows the average values computed for each benchmark program
for call subsumption in the SLG-WAM and in the YapTab engine.
By analyzing the results we can see that YapTab with RCS performs worse
than YapTab with subsumptive-based tabling in most cases, only the
programs \texttt{right\_first} and the \texttt{right\_last} program show better results,
while the \texttt{left\_first} program has comparable execution times.
In theory, these benchmarks should not run faster, but cache effects and other
conditions could affect positively the execution time of these programs.

The average values presented in Table~\ref{tbl:overhead_overview} show that
RCS is very competitive against XSB, because it is, in average, 21\% slower than RCS. More
importantly, when comparing RCS to YapTab with traditional call subsumption, RCS performs 5\% slower,
which shows that RCS adds a very small overhead when executing programs that do not benefit from the
new evaluation model.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \textbf{SLG-WAM} & \textbf{YapTab} \\
    & \textbf{\textit{\small{Sub / Retro}}} & \textbf{\textit{\small{Sub / Retro}}} \\
   \hline
   \hline
double\_first & 1.11 & 0.89 \\
double\_last & 1.03 & 0.87 \\
left\_first & 1.29 & \textbf{1.00} \\
left\_last &  1.26  & 0.92 \\
right\_first & 0.92 & \textbf{1.06} \\
right\_last & 1.00 & \textbf{1.08} \\
samegen & 0.78 & 0.91 \\
reach\_first  &  1.801  & 0.93 \\
reach\_last  &  1.703  & 0.92 \\
\hline
\hline
\textit{Average} &  1.21 &  0.95 \\
\hline
\hline
\end{tabular}
\caption{Average overhead values for programs do not take advantage of RCS.}
\label{tbl:overhead_overview}
\end{table}

\input{tables_retro_overhead_model}
\input{tables_retro_overhead_tst}

\subsection{RCS Gains}

In this section, we present experimental results using retroactive-based tabling on programs that
can benefit from it, i.e., programs where some general subgoal is called after a more specific subgoal.
The programs we used for these experiments are the following:

\begin{itemize}
   \item The programs \texttt{left\_first}, \texttt{left\_last},
   \texttt{double\_first} and \texttt{double\_last} with the query goal `\texttt{?-~path(X,1).}'.
   When calling the subgoal \texttt{path(X,1)}, the subgoal
   \texttt{path(X,Y)} is called, which prunes the first subgoal.
   
   \item The \texttt{genome/1} program with the query goal `\texttt{?-~genome(X).}'.
   
   \item The two versions of the \texttt{reach/2} program with the following pair of queries/relation graphs:

   \begin{itemize}
      \item sieve: `\texttt{?-~reach(sieve\_0(5,4,27,end),~par(A,~B,~C,~D)).}'.
      \item leader: `\texttt{?-~reach(systemLeader\_0(5,end),~par(D,~E,~A,~B)).}'.
      \item iproto: `\texttt{?-~reach(iproto\_0(\_,\_,end),imain\_7\_0(A,~B,~C,~D,~E)).}'.
   \end{itemize}
   
   \item The \textbf{flora} program with the query goal `\texttt{?-~\'\_\$\_\$\_flora\_isa\_rhs\'(\_,direct).}'.
   
   \item The \textbf{fib} program with the following parameters: 30, 32, 35, and 36. We use the following
   query goal: `\texttt{?-~a(X),~p(Y,~Z).}'.
   
   \item The \textbf{big} program with the following parameters (number of subgoals to prune): 5, 10, and 20.
   We use the query goal `\texttt{?-~a(X).}'
\end{itemize}

As the tables of the previous section, we present the results with the execution time of the retroactive-based
engine, along with the relative results for the SLG-WAM and YapTab engines, with both variant and subsumptive checks.
In tables~\ref{tbl:results_detail_gain_tst}, \ref{tbl:results_detail_gain_model} and \ref{tbl:results_detail_gain_fib}
we show the detailed results of the experiments. Table~\ref{tbl:results_gain_overview} presents the average values of
each program.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \textbf{SLG-WAM} & \textbf{YapTab} \\
    & \textbf{\textit{\small{Sub / Retro}}} & \textbf{\textit{\small{Sub / Retro}}} \\
   \hline
   \hline
double\_first & 1.20 & \textbf{1.13} \\
double\_last & 1.14 & \textbf{1.14} \\
genome & 0.75 & 0.92 \\
left\_first & 1.19 & 0.95 \\
left\_last & 0.76  & 0.88 \\
reach\_first  & 3.28 & \textbf{1.63} \\
reach\_last  & 3.25 & \textbf{1.85} \\
flora & 0.29 & \textbf{1.17} \\
fib & 1.75 & \textbf{2.19} \\
big & 17.18 & \textbf{13.60} \\
\hline
\hline
\end{tabular}
\caption{Average benchmarks results for programs that take advantage of RCS.}
\label{tbl:results_gain_overview}
\end{table}

For the \texttt{path/2} we should make two distinctions. The versions where the recursive clause is the first
clause (\texttt{left\_first} and \texttt{double\_first}) and the versions with the recursive last as the second
clause (\texttt{left\_last} and \texttt{double\_last}). In the \texttt{first} versions, the specific subgoal
generates answers before reaching the general subgoal, while in the \texttt{last} versions, the general subgoal
is found right at the beginning. Therefore, the \texttt{first} versions should, in principle, attain more performance,
because they waste less time executing the subsumed subgoal. The results for the \texttt{left} versions of the
\texttt{path/2} program attest this, with speedups of 0.95 and 0.88 for the \texttt{left\_first} and
\texttt{left\_last} versions, respectively. Surprisingly, the \texttt{double} version has the opposite behavior.

Another important conclusion we can make from the results is that RCS may not show performance gains, even if the
programs can, in theory, take advantage of it. In our experiments, only the programs \texttt{double\_left},
\texttt{double\_last}, \texttt{reach\_left} and \texttt{reach\_last} show, in average, performance improvements.

We argue that the speedup of using RCS is highly dependent on the nature of the program.
The \texttt{left} version
of the \texttt{path/2} program, for example, does not show improvements because what we gain from pruning the
execution of simple \texttt{edge/2} facts does not offset the cost of using the STST to retrieve relevant answers
for the subsumed subgoal and
the cost of pruning the computation itself. In other words, using subsumptive-based tabling for this program is
advantageous because the cost of executing predicate clauses is less than maintaining the time stamped index.
Therefore, the time that would be wasted by executing pruned code should be bigger than the time wasted
on manipulating the STST.

The model checking programs, \texttt{reach\_left} and \texttt{reach\_last}, are much more complex than the \texttt{path/2}
programs, and thus show much larger improvements. For the \texttt{reach\_last} program with the \texttt{leader} model,
we obtain a good speedup of 2.03. Also, the \texttt{flora} program also shows improvements with a speedup of 1.17 over
traditional call subsumption in YapTab. These complex benchmarks show that using RCS has the potential to speedup the
execution on this type of programs.

The \textbf{fib} program shows a considerable average speedup of 2.19 for YapTab with call subsumption.
We can see that, for different parameters, the speedup is maintained because the execution time of the pruned subsumed
subgoal is more or less equal to that of the subsuming subgoal. For the \textbf{big} program, where multiple
subsumed subgoals are pruned, we can see that, as the number of pruned subsumed subgoals increase, the speedup
also increases, reaching a maximum of 21.29 for YapTab. An interesting observation for the \textbf{big} program is
that the execution time for retroactive-tabling stays the same, even if the number of subsumed subgoals to prune
increase.

Even if the \texttt{path/2} programs, in average, do not show improvements, for some data sets
these programs can obtain good speedups. For example, the binary tree configuration can reach speedups of 1.62 and 1.63. The grid configuration can reach speedups of 1.10 for the \texttt{left\_first} benchmark, and the \texttt{genome/1}
program reaches a speedup of 2.0 for the grid graph.

\input{tables_gain_model}
\input{tables_gain_fib}
\input{tables_gain_tst}

\section{STST Table Space Analysis}

In the Single Timed Stamped Trie table space, the answers for all the subgoals
of a predicate are stored in a single answer trie. While advantageous, all arguments of
the answers must be stored in the trie. In this section, we experiment with programs
where this property is stressed to measure the overhead in terms of space and time, when
the the load operation is more expensive and the store operation needs to insert more terms in
the trie than what is needed to complete the computation.

For this, we used the \texttt{path/2} program. We transformed, both the program and data sets,
to use a functor term in each argument, instead of simple integers. For example, an
\texttt{edge(3,4)} fact is transformed into \texttt{edge(f(3), f(4))}. The updated version
of the \texttt{left\_first} program is exemplified in Figure~\ref{fig:converted_path}.
We experimented the query goal `\texttt{?-~path(f(X),f(Y))}' with all the combinations of
the \texttt{path/2} versions and the graph data sets.

\begin{figure}[ht]
\begin{Verbatim}
path(f(X),f(Y)) :- path(f(X),f(Z)), edge(f(Z),f(Y)).
path(f(X),f(Y)) :- edge(f(X),f(Y)).
\end{Verbatim}
\caption{Transformed \texttt{path/2} program.}
\label{fig:converted_path}
\end{figure}

\subsubsection{Time Analysis}

Table~\ref{tbl:results_detail_stst} present the detailed results concerning execution time for each
program and data set.
The average results are presented in Table~\ref{tbl:results_average_stst} and compare the STST to
tabling with variant and subsumptive checks. From the results,
we see that, in average, the STST table space has an overhead of 20\%, which is considerable when
compared to the overhead of 5\% found early on this chapter.

The consumption of answers by consumers and the insertion of new answers by generators into the table
space are the primary causes for the overhead in RCS for these benchmarks. The programs with the
worst overhead are \texttt{double\_first} and \texttt{double\_last}, with 33\% and 35\% of overhead
against traditional call subsumption. These programs also create the higher number of consumers,
both variant consumers and subsumed consumers than any other benchmark in this experiment.
The \texttt{right\_first} and \texttt{right\_last} are the only programs where only subsumed
consumers are created, and they have an overhead of 7\% and 11\%, respectively, which are the lowest
overhead values. In the programs \texttt{left\_first} and \texttt{left\_last}, only one variant
consumer is allocated, however they perform worse than the \texttt{right} versions.

We thus argue that the number of consumer nodes can greatly reduce the
applicability and performance of the STST table space when the operation of loading an answer
from the trie is more expensive. While this situation is disadvantageous, execution time can
be reduced when another subgoal call appears (for example \texttt{path(X,Y)}), where its possible to
reuse the answers from the table instead of executing the predicate clauses, thus providing an
elegant solution to the problem of incomplete tables.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \multicolumn{2}{c}{\textbf{YapTab}} \\
    & \textbf{\textit{\small{Var / Retro}}} & \textbf{\textit{\small{Sub / Retro}}} \\
   \hline
   \hline
double\_first & 0.52 & 0.67 \\
double\_last & 0.51 & 0.65 \\
left\_first & 0.87 & 0.86 \\
left\_last & 0.78 & 0.77 \\
right\_first & 0.85 & 0.93 \\
right\_last & 0.92 & 0.89 \\
\hline
\hline
\textit{Average} & 0.74 &  0.80 \\
\hline
\hline
\end{tabular}
\caption{Average results for the query goal `\texttt{?-~path(f(X),f(Y))}'.}
\label{tbl:results_average_stst}
\end{table}

\input{tables_stst_overhead}

\subsubsection{Space Analysis}

We executed the previous benchmarks and measured the number of answer trie nodes for each program.
The detailed results are presented in Table~\ref{tbl:results_detail_stst_space}. In this table
we show the number of answer trie nodes allocated for the RCS execution and then the relative number
of trie nodes for the variant and subsumptive-based executions, all for YapTab.
The programs \texttt{left}, \texttt{right} and \texttt{double} are the left, right and double versions
of the \texttt{path/2} program. Note that we do not need to differentiate between \texttt{first} and
\texttt{last} versions, because they generate the same number of trie nodes. For the data sets, we used
graph configurations with different sizes.

Table~\ref{tbl:results_average_stst_space} presents the average results of RCS against variant and
subsumptive-based engines. From these results we can see that the STST table space is 89\% more efficient
than the variant table space. In the \texttt{double} program, these differences are augmented because in the
variant engine there are more generator subgoal calls and thus more answer tries are created.

When comparing to the subsumptive-based engine, the STST table space only stores 4.2\% more trie nodes,
even if the \texttt{f/1} functor terms need to be stored. This is easily understandable because
the first functor term is only represented once, at the top of the trie, and then there is one second functor
for each source node number in the graph, therefore, the number of functors stored is insignificant when
compared to the rest of terms that are stored in the trie. Also note that, for the \texttt{double} benchmark,
the data sets used are small compared to the other data sets used in other benchmarks, and the space overhead
is more significant (in the worst case 18\%). We thus argue that in the worst case,
the extra space needed to store terms in the single answer trie get more insignificant as more terms
that are directly related to the query goal are stored in the trie.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \multicolumn{2}{c}{\textbf{YapTab}} \\
    & \textbf{\textit{\small{Var / Retro}}} & \textbf{\textit{\small{Sub / Retro}}} \\
   \hline
   \hline
double & 2.72892 & 0.90149 \\
left & 0.99258 & 0.99258 \\
right & 1.95122 & 0.97906 \\
\hline
\hline
\textit{Average} & 1.89091 &  0.95771 \\
\hline
\hline
\end{tabular}
\caption{Average space results for the query goal `\texttt{?-~path(f(X),f(Y))}'.}
\label{tbl:results_average_stst_space}
\end{table}

\input{tables_stst_space}