This chapter presents a detailed performance analysis of our two subsumption-based tabling engines
that we have developed. We divided this chapter into four sections. The first section describes the
set of tabled benchmark programs used. The second
section evaluates the engine supporting traditional call subsumption that was implemented by integrating the
Time-Stamped Tries algorithms and data structures from XSB into Yap. This includes analyzing the memory gains
of call subsumption by measuring the size of the answer tries and comparing it to variant-based tabling.
In the third section, we evaluate the retroactive-based tabling engine for programs that do not benefit
from the new mechanisms and for programs that can take advantage of this new evaluation method. Finally,
in the fourth section we evaluate the STST table space overhead in a potentially not so good scenario,
where the operations of loading and storing answers are more expensive than usual.

\section{Benchmark Programs}

In order to assess the performance of our tabling engines we used various programs and data sets.
We next briefly describe the programs used (see Appendix~\ref{app:code} for more details).

\begin{description}
   
   \item[path:] This program computes the reachability between two nodes in a graph.
   Connections between two nodes are represented by \texttt{edge/2} facts.
   We used the following graph configurations in our tests: \textbf{tree}, a
   binary tree; \textbf{chain}, a chain of nodes; \textbf{cycle}, a chain of nodes, where the
   last node connects with the first one; \textbf{pyramid}, a pyramid like configuration;
   and \textbf{grid}, where nodes are connected in a grid-like fashion.
   For the \texttt{path/2} predicate itself, we used 6 different versions: \texttt{left\_first},
   \texttt{left\_last}, \texttt{right\_first}, \texttt{right\_last},
   \texttt{double\_first}, and \texttt{double\_last}.
    
   \item[samegen:] The \texttt{samegen/2} predicate solves the same generation problem.
   For this program, we used the configurations described above for \texttt{path}.
   
   \item[genome:] This program computes the set of nodes that are reachable by nodes 1 and 2 in a graph.
   This is an interesting problem, since it creates lots of subsumed consumers when using
   call subsumption. We also used the same configurations described above for \texttt{path}.
   To compute reachability this program uses a \texttt{left\_last} version.
   
   \item[reach:] The \texttt{reach/2} predicate computes the reachability in a relation graph for a set of
   model specifications. The benchmark is actually a set of programs originally taken from the
   XMC project~\cite{RamakrishnanCR-00}, which is a model checker implemented
   atop the XSB system. We used two variants of the \texttt{reach/2} predicate,
   \texttt{reach\_first} and \texttt{reach\_last}.
   The following relation graphs where used:
   
      \begin{description}
         
         \item[sieve:] \emph{sieve} specification defined for 5 processes and 4 overflow prime numbers.
         
         \item[leader:] \emph{leader election} specification defined for 5 processes.
         
         \item[iproto:] \emph{i-protocol} specification defined for a correct version with a huge window size.

      \end{description}
      
   \item[flora:] This program was generated by an object-oriented knowledge base language and application 
               development environment known as FLORA-2 \cite{Yang-00} \footnote{http://flora.sourceforge.net}.
               
   \item[fib:] This program uses a \texttt{fib/2} predicate to compute the Fibonacci number of a given
   parameter which allows to benchmark the pruning of one subsumed subgoal.
   
   \item[big:] This program also uses the \texttt{fib/2} predicate, but instead of one, multiple subsumed subgoals
   are called and pruned. As a parameter, we can input the number of subsumed subgoals to call and prune.
   
\end{description}

Again, Note that the relevant parts of the code for these programs are presented in Appendix~\ref{app:code}.

The environment for our experiments was an Intel Core(TM) 2 Quad 2.66 GHz with 4 GBytes of
memory and running the Linux kernel 2.6.31 with YapTab 6.0.3 and XSB Prolog 3.2.
The scheduling strategy used by default was batched scheduling.

\section{Traditional Call Subsumption with TST}

In this section, we first evaluate the performance of YapTab against the SLG-WAM,
by comparing the gains obtained by using call subsumption instead of variant checks.
In the second part of this section, we measure the impact in terms of space of using call subsumption.
For this, we compared the number of answer trie nodes created by using variant checks and by using subsumptive
checks.

\subsection{Performance Evaluation}

In order to compare the YapTab tabling engine with the SLG-WAM we used the following programs:

\begin{itemize}
   \item The \texttt{path/2} program with all the combinations of
   versions and data sets and the query goal `\texttt{?-~path(X,Y).}'.
   
   \item The \texttt{samegen/2} program with all data sets and the query goal `\texttt{?-~samegen(X,Y).}'.
   
   \item The \texttt{genome/1} program with all the data sets and the query goal `\texttt{?-~genome(X).}'.
   
   \item The two versions of the \texttt{reach/2} program with the following queries for each relation graphs:

   \begin{itemize}
      \item sieve: `\texttt{?-~reach(sieve\_0(5,4,27,end),Y).}'.
      \item leader: `\texttt{?-~reach(systemLeader\_0(5,end),Y).}'.
      \item iproto: `\texttt{?-~reach(iproto\_0(\_,\_,end),Y).}'.
   \end{itemize}

\end{itemize}

For each benchmark, we used variant-based tabling and then subsumption-based tabling.
Next, we computed the execution time and compared the speedups obtained ($T_{variant} / T_{subsumptive}$) for
each engine. The times presented next are the average of 3 runs. Given that YapTab's implementation
is largely based on XSB's code to implement the subsumption mechanisms,
we expect the speedups to be very similar. Some potential differences between them will arise because
of certain characteristics, namely: the way they implement the tabling algorithms, the WAM engine itself,
the compiled trie code, and the handling of answer templates.

Table~\ref{tbl:results_overview} summarizes the average speedups obtained for each program,
while Tables~\ref{tbl:result_detail_path} and \ref{tbl:result_detail_others}
show the full details, with times and speedups for YapTab and SLG-WAM.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \textbf{SLG-WAM} & \textbf{YapTab} \\
    & \textbf{\textit{\small{Average Speedup}}} & \textbf{\textit{\small{Average Speedup}}} \\
   \hline
   \hline
left\_first & 0.78 & \textbf{1.02} \\
left\_last & 0.77  & \textbf{0.96} \\
right\_first & \textbf{1.01} & \textbf{1.01} \\
right\_last & 0.94 & \textbf{1.07} \\
double\_first & 1.37 & \textbf{1.48} \\
double\_last & 1.31 & \textbf{1.40} \\
samegen & \textbf{339.76} & 1.03 \\
genome & 559.54 & \textbf{648.51} \\
reach\_first  & \textbf{0.96} & 0.94 \\
reach\_last  & \textbf{0.97} & 0.90 \\
\hline
\hline
\end{tabular}
\caption{Average speedups for call subsumption in SLG-WAM and YapTab.}
\label{tbl:results_overview}
\end{table}

The first thing we note is that,  YapTab has a better speedup than SLG-WAM in 6 benchmarks, while in 3 of
them SLG-WAM wins. In most of the benchmarks, the speedups for the two engines are very similar, which proves
that our integration efforts were  largely successful. However, for the \texttt{samegen} benchmark the
speedups are not very similar at all, the SLG-WAM engine has an average speedup of 339.76
and YapTab only 1.03. This happens because the performance of the variant-based version of SLG-WAM
performs very poorly against YapTab, which explains such big differences (see Table~\ref{tbl:result_detail_others}
for details).

The programs \texttt{left\_first} and \texttt{left\_last} do not generate any subsumed consumer,
therefore they are good benchmarks to assess the overhead of using the subsumption mechanisms. For YapTab,
the overhead is minimal with an average speedup of 0.96 for the \texttt{left\_last} program. Surprisingly, for the
\texttt{left\_first} program the speedup is 1.02, that is, the subsumptive-based engine performed better
than the variant-based engine. The SLG-WAM behavior for these two programs is clearly worst, with an average
speedup of 0.78 and 0.77 for \texttt{left\_first} and \texttt{left\_last}, respectively.

The programs \texttt{right\_first} and \texttt{right\_last} do generate subsumed consumers,
as many as the number of \texttt{edge/2} facts. Notably, only YapTab
achieves an average speedup bigger than 1 for both programs. These programs show poor speedups because simple facts
are faster to evaluate than to use the time stamped trie to collect relevant answers.
In particular, the binary tree graph configuration with the \texttt{right\_first} and \texttt{right\_last} programs
in YapTab has a very poor speedup of 0.93 and 0.33, respectively, which is clearly influencing the average speedups.
(see Table~\ref{tbl:result_detail_path}).
In the \texttt{right\_first} benchmark, the time stamped index is created right at
the beginning of the program, when the time stamped trie is still empty, and maintained thereafter, but,
in the case of the \texttt{right\_last} program, the indices are only created when the recursive
clause is executed, and at that time the trie already contains a considerable amount of answers.
We thus modified the subsumptive
engine to create the time stamp index from the beginning, and the \texttt{right\_last} program had considerable
better results. Therefore, we argue that the lazy creation of the time stamped index can affect considerably the
execution time, for programs where consumers appear when the answer trie already contains lots of answers.
The operation of creating the time stamped index
and sorting each trie node at each trie level can be very costly when a large number of nodes exist.
By experimentation we found that it seems more efficient to maintain the index as the answer trie is being expanded.

For the \texttt{double\_first} and \texttt{double\_last} we have attained speedups between 1.31 and 1.48
for both YapTab and SLG-WAM. These benchmarks are more computationally expensive given that they create more
dependencies. However, in subsumption-based tabling, the number of dependencies is reduced 
and thus no code is executed.

The \texttt{genome} program shows the best speedup results, with an impressive average speedup of 
648.51 for YapTab. In this program, the subgoal \texttt{path(2,X)} and \texttt{path(1,X)} are called
very early in the evaluation which makes all further subgoals calls to \texttt{path/2} that are subsumed by these
goals as consumers.

For the model checking programs, the results were not so good with close speedups for YapTab and SLG-WAM.

\input{tables_path}
\input{tables_model_checking}

\subsection{Memory usage}

In this section we measure the size of the table space, for the variant and the subsumptive engine.
As a metric, we use the number of allocated answer trie nodes across all answer tries.
For this, we used the following benchmarks:

\begin{itemize}
   \item The programs \texttt{left\_first}, \texttt{right\_first} and \texttt{double\_first} with the query `\texttt{?-~path(X,Y).}'. Note that there is no need to use
   the \texttt{last} versions of these programs, because they will result in the same number of answer trie nodes.
   
   \item The \texttt{samegen/2} program.
   
   \item The \texttt{genome/1} program.
   
   \item The \texttt{flora} benchmark.
\end{itemize}

Table~\ref{tbl:results_detail_space_sub} presents the results we obtained. From the table, we see that,
for some programs where no subsumed subgoals are called, the number of answer trie nodes created is
exactly same when compared to variant-based tabling. For example, in the programs \texttt{genome/1} and
\texttt{left\_first} we can observe this behavior. Please notice that, while the number of trie nodes is
the same, the size of the nodes in subsumption-based tabling (they have a \textbf{timestamp} field) make
the time stamped answer trie more costly in terms of memory used.

In the programs \texttt{right\_first} and \texttt{double\_first}, the number of answer trie nodes
is reduced in half. Using variant-based tabling, these programs generate a large number of subgoals,
that in subsumption-based tabling are consumers of the first called subgoal, thus creating more
answer tries for each one of these subgoals, and thus more answer trie nodes. As one would expect,
using subsumption-based tabling can reduce substantially the table space.

The \texttt{samegen/2} program presents a curious behavior for the \texttt{cycle} and \texttt{chain}
data sets. In these cases, the program generates an answer that subsumes all the other answers and
every other subgoal call is subsumed by the top subgoal, resulting in only 3 answer trie nodes created,
the root node, and the two nodes for the two terms of the general solution. Compared to tabling with
variant checks, this result in very good space savings.
The \texttt{flora} benchmark also shows a reduced table space size, which shows that subsumption-based
tabling can be successfully applied to complex programs, resulting in a largely reduced
table space.

\input{tables_sub_space}

\section{Performance Evaluation for RCS}

In this section we evaluate our retroactive-based tabling engine that we implemented on
top of YapTab. First, we start by assessing the overhead of using the new mechanisms that
support the RCS engine, namely: building the subgoal dependency tree, the STST table space,
and searching for running subsumed subgoals. In the second part of this section, we evaluate
the RCS engine with programs where specific subgoals are called before general subgoals, in
order to assess the advantages of the new mechanism.

\subsection{RCS Overhead}

To measure the overhead, we executed programs where general subgoals are always called before
subsumed subgoals (or not at all), therefore we can estimate the impact in the execution time
because the pruning techniques of RCS are not employed.
We used the following benchmarks:

\begin{itemize}
   \item Every combination of versions and data sets for the \texttt{path/2} program with the query `\texttt{?-~path(X,Y).}'.
   
   \item The query `\texttt{?-~samegen(X,Y).}' in the \texttt{samegen/2} program. All \texttt{path/2} data
   sets were used.
   
   \item The two versions of the \texttt{reach/2} program with the following queries for the relation graphs:

   \begin{itemize}
      \item sieve: `\texttt{?-~reach(sieve\_0(5,4,27,end),Y).}'.
      \item leader: `\texttt{?-~reach(systemLeader\_0(5,end),Y).}'.
      \item iproto: `\texttt{?-~reach(iproto\_0(\_,\_,end),Y).}'.
   \end{itemize}
\end{itemize}

We timed the execution of the benchmarks for the RCS engine and then the execution time
for the subsumptive and variant-based engines of both YapTab and SLG-WAM. All execution times are
the average of 3 runs. For each table,
we present the execution time of the RCS engine in milliseconds and the relative time
of the other engines by computing the value $T_{engine} / T_{RCS}$.
If the value is lesser than 1.0 then RCS performs worse, otherwise if the value is greater than
1.0 then RCS performs better. At the end of each table, we present the average values for
each engine.

Table~\ref{tbl:overhead_overview} shows the average overhead values computed for each benchmark program
for call subsumption in the SLG-WAM and in the YapTab engine. The detailed results are presented
in Tables~\ref{tbl:overhead_detail_tst} and \ref{tbl:overhead_detail_model}.
By analyzing the results we can see that YapTab with RCS performs worse
than YapTab with subsumptive-based tabling in most cases, and only the
programs \texttt{right\_first} and the \texttt{right\_last} program show better results,
while the \texttt{left\_first} and \texttt{left\_last} programs have very comparable execution times.
In theory, these benchmarks should not run faster, but cache effects and other
conditions could affect positively the execution time of these programs.

The average values presented in Table~\ref{tbl:overhead_overview} show that
RCS is very competitive against SLG-WAM, because SLG-WAM is in average 21\% slower than RCS. More
importantly, when comparing RCS to YapTab with traditional call subsumption, RCS performs 4\% slower,
which shows that RCS adds a very small overhead when executing programs that do not benefit from the
new evaluation model.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \textbf{SLG-WAM} & \textbf{YapTab} \\
    & \textbf{\textit{\small{Sub / Retro}}} & \textbf{\textit{\small{Sub / Retro}}} \\
   \hline
   \hline
   left\_first & 1.29 & 0.99 \\
   left\_last &  1.26  & 0.98 \\
   right\_first & 0.95 & \textbf{1.05} \\
   right\_last & 1.03 & \textbf{1.08} \\
   double\_first & 1.05 & 0.87 \\
   double\_last & 1.06 & 0.87 \\
   samegen & 0.75 & 0.90 \\
   reach\_first  &  1.78  & 0.96 \\
   reach\_last  &  1.73  & 0.96 \\
\hline
\hline
\textit{Average} &  1.21 &  0.96 \\
\hline
\hline
\end{tabular}
\caption{Average overhead values for programs do not take advantage of RCS.}
\label{tbl:overhead_overview}
\end{table}

\input{tables_retro_overhead_tst}
\input{tables_retro_overhead_model}

\subsection{RCS Gains}

In this section, we present experimental results using retroactive-based tabling on programs that
can benefit from it, i.e., programs where some general subgoal is called after a more specific subgoal.
The programs we used for these experiments are the following:

\begin{itemize}
   \item The programs \texttt{left\_first}, \texttt{left\_last},
   \texttt{double\_first} and \texttt{double\_last} with the query goal `\texttt{?-~path(X,1).}'.
   When calling the subgoal \texttt{path(X,1)}, the subgoal
   \texttt{path(X,Y)} is called, which prunes the first subgoal.
   
   \item The \texttt{genome/1} program with the query goal `\texttt{?-~genome(X).}'.
   
   \item The two versions of the \texttt{reach/2} program with the following pair of queries/relation graphs:

   \begin{itemize}
      \item sieve: `\texttt{?-~reach(sieve\_0(5,4,27,end),~par(A,~B,~C,~D)).}'.
      \item leader: `\texttt{?-~reach(systemLeader\_0(5,end),~par(D,~E,~A,~B)).}'.
      \item iproto: `\texttt{?-~reach(iproto\_0(\_,\_,end),imain\_7\_0(A,~B,~C,~D,~E)).}'.
   \end{itemize}
   
   \item The \textbf{flora} program with the query goal `\texttt{?-~\'\_\$\_\$\_flora\_isa\_rhs\'(\_,direct).}'.
   
   \item The \textbf{fib} program with the following parameters: 30, 32, 35, and 36. We use the following
   query goal: `\texttt{?-~a(X),~p(Y,~Z).}'.
   
   \item The \textbf{big} program with the following parameters (number of subgoals to prune): 5, 10, and 20.
   We use the query goal `\texttt{?-~a(X).}'
\end{itemize}

As the tables of the previous section, we present the results with the execution time of the retroactive-based
engine, along with the relative results for the SLG-WAM and YapTab engines, with both variant and subsumptive checks.
Execution times are an average of 3 runs.
In tables~\ref{tbl:results_detail_gain_tst}, \ref{tbl:results_detail_gain_model} and \ref{tbl:results_detail_gain_fib}
we show the detailed results of the experiments. Table~\ref{tbl:results_gain_overview} presents the average values of
each program.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \textbf{SLG-WAM} & \textbf{YapTab} \\
    & \textbf{\textit{\small{Sub / Retro}}} & \textbf{\textit{\small{Sub / Retro}}} \\
   \hline
   \hline
   
left\_first & 1.19 & 0.95 \\
left\_last & 1.23  & 0.90 \\
double\_first & 1.17 & \textbf{1.09} \\
double\_last & 1.14 & \textbf{1.10} \\
genome & 0.62 & 0.74 \\
reach\_first  & 3.52 & \textbf{1.76} \\
reach\_last  & 3.34 & \textbf{1.87} \\
flora & 0.29 & \textbf{1.17} \\
fib & 1.64 & \textbf{2.02} \\
big & 17.72 & \textbf{13.66} \\
\hline
\hline
\end{tabular}
\caption{Average benchmarks results for programs that take advantage of RCS.}
\label{tbl:results_gain_overview}
\end{table}

For the \texttt{path/2} we should make two distinctions. The versions where the recursive clause is the first
clause (\texttt{left\_first} and \texttt{double\_first}) and the versions with the recursive last as the second
clause (\texttt{left\_last} and \texttt{double\_last}). In the \texttt{first} versions, the specific subgoal
generates answers before reaching the general subgoal, while in the \texttt{last} versions, the general subgoal
is found right at the beginning. Therefore, the \texttt{first} versions should, in principle, attain more performance,
because they waste less time executing the subsumed subgoal. The results for the \texttt{left} versions of the
\texttt{path/2} program attest this, with speedups of 0.95 and 0.90 for the \texttt{left\_first} and
\texttt{left\_last} versions, respectively. Surprisingly, the \texttt{double} version has the opposite behavior.

Another important conclusion we can make from the results is that RCS may not show performance gains, even if the
programs can, in theory, take advantage of it. In our experiments, only the programs \texttt{left\_first},
\texttt{left\_last} and \texttt{genome} show, in average, worse performance.

We argue that the speedup of using RCS is highly dependent on the nature of the program.
The \texttt{left} version
of the \texttt{path/2} program, for example, does not show improvements because what we gain from pruning the
execution of simple \texttt{edge/2} facts does not offset the cost of using the STST to retrieve relevant answers
for the subsumed subgoal and
the cost of pruning the computation itself. In other words, using subsumptive-based tabling for this program is
advantageous because the cost of executing predicate clauses is less than maintaining the time stamped index.
Hence, for RCS to show improvements, the time that is wasted by executing pruned code should be greater than the
time spent manipulating the STST.

The model checking programs, \texttt{reach\_left} and \texttt{reach\_last}, are much more complex than the \texttt{path/2}
programs, and thus show much larger improvements. For the \texttt{reach\_last} program with the \texttt{leader} model,
we obtain a good speedup of 2.16. Also, the \texttt{flora} program also shows improvements with a speedup of 1.17 over
traditional call subsumption in YapTab. These complex benchmarks show that RCS has the potential to speedup the
execution on this type of programs.

The \textbf{fib} program shows a considerable average speedup of 2.02 for YapTab with call subsumption.
We can see that, for different parameters, the speedup is identical because the execution time of the pruned subsumed
subgoal is more or less equal to that of the subsuming subgoal. For the \textbf{big} program, where multiple
subsumed subgoals are pruned, we can see that as the number of pruned subsumed subgoals increases, the speedup
also increases, reaching a maximum of 21.98 for YapTab. An interesting observation for the \textbf{big} program is
that the execution time for retroactive-tabling stays the same, even if the number of subsumed subgoals to prune
increase.

Even if the \texttt{path/2} programs, in average, do not show considerable improvements, for some data sets
these programs can obtain good speedups, for example, the binary tree configuration can reach a speedup of 1.61.

\input{tables_gain_tst}
\input{tables_gain_model}
\input{tables_gain_fib}

\section{STST Table Space Analysis}

In the Single Timed Stamped Trie table space, the answers for all the subgoals
of a predicate are stored in a single answer trie. While advantageous, all arguments of
the answers must be stored in the trie. In this section, we experiment with programs
where this property is stressed to measure the overhead in terms of space and time, when
the the load operation is more expensive and the store operation needs to insert more terms in
the trie than what is needed to complete the computation.

For this, we used the \texttt{path/2} program. We transformed, both the program and data sets,
to use a functor term in each argument, instead of simple integers. For example, an
\texttt{edge(3,4)} fact is transformed into \texttt{edge(f(3), f(4))}. The updated version
of the \texttt{left\_first} program is exemplified in Figure~\ref{fig:converted_path}.
We experimented the query goal `\texttt{?-~path(f(X),f(Y))}' with all the combinations of
the \texttt{path/2} versions and the graph data sets.

\begin{figure}[ht]
\begin{Verbatim}
path(f(X),f(Y)) :- path(f(X),f(Z)), edge(f(Z),f(Y)).
path(f(X),f(Y)) :- edge(f(X),f(Y)).
\end{Verbatim}
\caption{Transformed \texttt{path/2} program.}
\label{fig:converted_path}
\end{figure}

\subsubsection{Time Analysis}

Table~\ref{tbl:results_detail_stst} present the detailed results concerning execution time for each
program and data set.
The average results are presented in Table~\ref{tbl:results_average_stst} and compare the STST to
tabling with variant and subsumptive checks. From the results,
we see that, in average, the STST table space has an overhead of 20\%, which is considerable when
compared to the overhead of 5\% found early on this chapter.

The consumption of answers by consumers and the insertion of new answers by generators into the table
space are the primary causes for the overhead in RCS for these benchmarks. The programs with the
worst overhead are \texttt{double\_first} and \texttt{double\_last}, with 33\% and 35\% of overhead
against traditional call subsumption. These programs also create the higher number of consumers,
both variant consumers and subsumed consumers than any other benchmark in this experiment.
The \texttt{right\_first} and \texttt{right\_last} are the only programs where only subsumed
consumers are created, and they have an overhead of 7\% and 11\%, respectively, which are the lowest
overhead values. In the programs \texttt{left\_first} and \texttt{left\_last}, only one variant
consumer is allocated, however they perform worse than the \texttt{right} versions.

We thus argue that the number of consumer nodes can greatly reduce the
applicability and performance of the STST table space when the operation of loading an answer
from the trie is more expensive. While this situation is disadvantageous, execution time can
be reduced when another subgoal call appears (for example \texttt{path(X,Y)}), where its possible to
reuse the answers from the table instead of executing the predicate clauses, thus providing an
elegant solution to the problem of incomplete tables.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \multicolumn{2}{c}{\textbf{YapTab}} \\
    & \textbf{\textit{\small{Var / Retro}}} & \textbf{\textit{\small{Sub / Retro}}} \\
   \hline
   \hline
   left\_first & 0.87 & 0.86 \\
   left\_last & 0.78 & 0.77 \\
   right\_first & 0.85 & 0.93 \\
   right\_last & 0.92 & 0.89 \\
double\_first & 0.52 & 0.67 \\
double\_last & 0.51 & 0.65 \\

\hline
\hline
\textit{Average} & 0.74 &  0.80 \\
\hline
\hline
\end{tabular}
\caption{Average results for the query goal `\texttt{?-~path(f(X),f(Y))}'.}
\label{tbl:results_average_stst}
\end{table}

\input{tables_stst_overhead}

\subsubsection{Space Analysis}

We executed the previous benchmarks and measured the number of answer trie nodes for each program.
The detailed results are presented in Table~\ref{tbl:results_detail_stst_space}. In this table
we show the number of answer trie nodes allocated for the RCS execution and then the relative number
of trie nodes for the variant and subsumptive-based executions, all for YapTab.
The programs \texttt{left}, \texttt{right} and \texttt{double} are the left, right and double versions
of the \texttt{path/2} program. Note that we do not need to differentiate between \texttt{first} and
\texttt{last} versions, because they generate the same number of trie nodes. For the data sets, we used
graph configurations with different sizes.

Table~\ref{tbl:results_average_stst_space} presents the average results of RCS against variant and
subsumptive-based engines. From these results we can see that the STST table space is 89\% more efficient
than the variant table space. In the \texttt{double} program, these differences are augmented because in the
variant engine there are more generator subgoal calls and thus more answer tries are created.

When comparing to the subsumptive-based engine, the STST table space only stores 4.2\% more trie nodes,
even if the \texttt{f/1} functor terms need to be stored. This is easily understandable because
the first functor term is only represented once, at the top of the trie, and then there is one second functor
for each source node number in the graph, therefore, the number of functors stored is insignificant when
compared to the rest of terms that are stored in the trie. Also note that, for the \texttt{double} benchmark,
the data sets used are small compared to the other data sets used in other benchmarks, and the space overhead
is more significant (in the worst case 18\%). We thus argue that in the worst case,
the extra space needed to store terms in the single answer trie get more insignificant as more terms
that are directly related to the query goal are stored in the trie.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \multicolumn{2}{c}{\textbf{YapTab}} \\
    & \textbf{\textit{\small{Var / Retro}}} & \textbf{\textit{\small{Sub / Retro}}} \\
   \hline
   \hline
   left & 0.99258 & 0.99258 \\
   right & 1.95122 & 0.97906 \\
double & 2.72892 & 0.90149 \\
\hline
\hline
\textit{Average} & 1.89091 &  0.95771 \\
\hline
\hline
\end{tabular}
\caption{Average space results for the query goal `\texttt{?-~path(f(X),f(Y))}'.}
\label{tbl:results_average_stst_space}
\end{table}

\input{tables_stst_space}