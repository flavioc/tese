This chapter presents a detailed performance analysis of the two subsumption-based tabling engines
that we have developed. We divided this chapter into four sections. The first section describes the
set of tabled benchmark programs that we have used to assess the performance of execution. The second
section evaluates the engine with traditional call subsumption that was implemented by integrating the
Time-Stamped Tries algorithms and data structures from XSB into Yap. In the next section, we evaluate
the retroactive-based tabling engine for programs that do not benefit from the new mechanisms and
for programs that take advantage of the new evaluation method. The final, and fourth, section describes
the advantages and disadvantages of the three engines, variant, subsumptive or retroactive to
compute the the reachability between two nodes in a graph.

\section{Benchmark Programs}

In order to assess the performance of our tabling engines we used various programs and data sets.
The following list describes the programs used:

\begin{description}
   
   \item[path:] The \texttt{path/2} program computes the reachability between two nodes in a graph.
   Connections between two nodes are represented by \texttt{edge/2} facts.
   We used the following graph configurations in our tests: \textbf{tree}, graph is a
   binary tree; \textbf{chain}, graph is chain of nodes; \textbf{cycle}, graph is a chain, but
   last node on the chain connects with the first node; \textbf{pyramid}, a pyramid configuration;
   and \textbf{grid}, where nodes are connected in a grid-like fashion.
   For the \texttt{path/2} program itself, we used 6 different versions: \texttt{left\_first},
   \texttt{left\_last}, \texttt{right\_first}, \texttt{right\_last},
   \texttt{double\_first}, and \texttt{double\_last}.
    
   \item[samegen:] The \texttt{samegen/2} predicate solves the same generation problem.
   For this test, we used the same data sets of the \texttt{path/2} program.
   
   \item[genome:] This program computes the set of nodes that are reached by node 1 and node 2.
   This is an interesting problem, since it creates lots of subsumed consumers when using
   call subsumption. We also used the same data sets of the \texttt{path/2} program.
   To compute reachability this program uses the \texttt{left\_last} program.
   
   \item[reach:] The \texttt{reach/2} computes the reachability in a relation graph for a set of
   specifications. The benchmark is actually a set of programs originally taken from the
   XMC project~\cite{system-xmc,RamakrishnanCR-00}, which is a model checker implemented
   atop the XSB system. We used two variants of the \texttt{reach/2} predicate,
   \texttt{reach\_first} and \texttt{reach\_last}.
   The following relation graphs are used:
   
      \begin{description}
         
         \item[sieve:] \emph{sieve} specification defined for 5 processes and 4 overflow prime numbers.
         
         \item[leader:] \emph{leader election} specification defined for 5 processes.
         
         \item[iproto:] \emph{i-protocol} specification defined for a correct version with a huge window size.

      \end{description}
\end{description}

Note that the code for these programs is presented in Appendix~\ref{app:code}.

In the next tables, execution time is measured in milliseconds and all benchmarks
are executed to find all solutions for the problem at hand. The scheduling strategy used
by default is batched scheduling.
The environment for our experiments was an Intel Core(TM) 2 Quad 2.66 GHz with 4 GBytes of
memory and running the Linux kernel 2.6.31 with YapTab 6.0.3 and XSB Prolog 3.2.

\section{Performance Evaluation for YapTab\_TST}

In order to evaluate the YapTab\_TST tabling engine we used the following programs:

\begin{itemize}
   \item The \texttt{path/2} program with the query `\texttt{?-~path(X,Y)}' with all the combinations of
   versions and data sets.
   \item The \texttt{genome/1} program with different data sets. The query used was `\texttt{?-~genome(X)}'.
   
   \item The query `\texttt{?-~samegen(X,Y)}' in the \texttt{samegen/2} program. All \texttt{path/2} data
   sets were used.
   
   \item The two versions of the \texttt{reach/2} program with the following queries for the relation graphs:

   \begin{itemize}
      \item sieve: `\texttt{?-~reach(sieve\_0(5,4,27,end),Y)}'.
      \item leader: `\texttt{?-~reach(systemLeader\_0(5,end),Y)}'.
      \item iproto: `\texttt{?-~reach(iproto\_0(\_,\_,end),Y)}'.
   \end{itemize}

\end{itemize}

We compared the performance of the SLG-WAM and YapTab with traditional call subsumption.
For each benchmark, we used variant-based tabling and then subsumption-based tabling.
Next, we compared the values of each engine and calculated the speedup ($T_{variant} / T_{subsumptive}$) for
each engine. Given that YapTab\_TST uses a great deal of code from XSB to implement the subsumption mechanisms,
we expect the speedups to be very similar. Some potential differences between them will arise because
of certain characteristics, namely: the way they implement the tabling algorithms, the WAM engine itself,
the compiled trie code, and the handling of answer templates.

Tables~\ref{tbl:result_path_detail1}, \ref{tbl:result_path_detail2} and \ref{tbl:result_model_detail}
show the detailed performance tests, with times and speedups for YapTab and SLG-WAM for each program.
Table~\ref{tbl:results_overview} presents the average speedups for each program taken from the
detailed benchmarks.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \textbf{SLG-WAM} & \textbf{YapTab} \\
    & \textbf{\textit{\small{Speedup}}} & \textbf{\textit{\small{Speedup}}} \\
   \hline
   \hline
double\_first & 1.25 & \textbf{1.46} \\
double\_last & 1.34 & \textbf{1.35} \\
genome & \textbf{596.17} & 595.55 \\
left\_first & 0.76 & \textbf{0.95} \\
left\_last & 0.76  & \textbf{1.00} \\
right\_first & \textbf{1.03} & 0.92 \\
right\_last & \textbf{1.07} & 0.98 \\
samegen & \textbf{292.66} & 0.93 \\
reach\_first  & \textbf{0.95} & 0.89 \\
reach\_last  & \textbf{0.98} & 0.95 \\
\hline
\hline
\end{tabular}
\caption{Benchmarks for call subsumption in SLG-WAM and YapTab.}
\label{tbl:results_overview}
\end{table}

The first thing we note is that, in 4 benchmarks
YapTab has a better speedup than SLG-WAM, while in 6 of them SLG-WAM wins. While SLG-WAM wins here,
the speedups for the two engines are very similar, which proves that our integration
efforts of the Time-Stamped Tries approach to subsumption-based tabling in YapTab was largely successful.
There is one benchmark were this observation does not hold, which is the \texttt{samegen} benchmark.
The SLG-WAM engine has an average speedup of 292.66 and YapTab only 0.93 because the performance
of the variant-based version of SLG-WAM performs very poorly against YapTab which explains such big differences.

The programs \texttt{left\_first} and \texttt{left\_last} do not generate any subsumed consumer,
therefore they are good benchmarks to assess the overhead of using subsumption mechanisms. For YapTab,
the overhead is minimal with a speedup of 0.95 for the \texttt{left\_first}. Surprisingly, for the
\texttt{left\_last} program the speedup is equal to 1, that is, both variant-based and subsumptive-based
engines have the same execution time. SLG-WAM has more overhead, with 0.76 for both programs.

The programs \texttt{right\_first} and \texttt{right\_last} do generate subsumed consumers,
as many as \texttt{edge/2} facts. Notably, only the SLG-WAM with the \texttt{right\_first} program
achieves a speedup bigger than 1. Why there are no speedups for these programs is that simple facts
are faster to evaluate than to use the time stamped trie to collect relevant answers.
In YapTab, the binary tree graph configuration with the \texttt{right\_last} program has a very poor speedup
of 0.31 when compared to a speedup of 0.77 when used with the \texttt{right\_first} program.
In the \texttt{right\_first} benchmark, the time-stamped indices are created right at
the beginning of the program when the time-stamped trie is still empty, but, in the case of the \texttt{right\_last}
program, the indices are only created when the recursive clause is executed, when the trie already contains
a considerable amount of answers. We modified the subsumptive engine to create time-stamped indices from the
beginning, and the \texttt{right\_last} program had considerable better results. Therefore, we argue that the
lazy creation of time-stamped indices can affect considerably the execution time. For programs where subsumption
happens, the benchmarks may perform worse if a consumer appears when the answer trie already contains lots of answers.

For the \texttt{double\_first} and \texttt{double\_last} we have attained speedups between 1.25 and 1.46
for each configuration. These benchmarks are more computationally expensive given that they create more
dependencies. These dependencies diminish in subsumption-based tabling because only new consumers are created
and code is not executed.

The \texttt{genome} program attained very good speedup results, with
595.55 for YapTab. In this program, the subgoal \texttt{path(2,X)} and \texttt{path(1,X)} are called
very early in the evaluation and further subgoals calls to \texttt{path/2} that are subsumed by these
goals are consumers.

For the model checking programs, the results were not so good for subsumption-based evaluation with
identical speedups for YapTab and SLG-WAM.

\input{tables_model_checking}
\input{tables_path}

\section{Performance Evaluation for RCS}

In this section we evaluate our retroactive-based tabling engine that we implemented on
top of YapTab. First, we start by assessing the overhead of using the new mechanisms that
support the RCS engine, namely: building the subgoal dependency tree, the STST table space,
and searching for running subsumed subgoals. In the second part of this section, we evaluate
the RCS engine with programs where specific subgoals are called before general subgoals, in
order to assess the advantages of the new mechanism.

\subsection{RCS Overhead}

To measure the overhead, we executed programs where general subgoals are always called before
subsumed subgoals (or not at all), therefore we can estimate the impact in the execution time
because the pruning techniques of RCS are not employed.
We use the following benchmarks:

\begin{itemize}
   \item Every combination of versions and data sets for the \texttt{path/2} program with the query `\texttt{?-~path(X,Y)}'.
   
   \item The query `\texttt{?-~samegen(X,Y)}' in the \texttt{samegen/2} program. All \texttt{path/2} data
   sets were used.
   
   \item The two versions of the \texttt{reach/2} program with the following queries for the relation graphs:

   \begin{itemize}
      \item sieve: `\texttt{?-~reach(sieve\_0(5,4,27,end),Y)}'.
      \item leader: `\texttt{?-~reach(systemLeader\_0(5,end),Y)}'.
      \item iproto: `\texttt{?-~reach(iproto\_0(\_,\_,end),Y)}'.
   \end{itemize}
\end{itemize}

Note that we do not use the \texttt{genome/2} benchmark because subsumed subgoals are
pruned by subsuming subgoals in this program.

We timed the execution of the benchmarks for the RCS engine and then the execution time
for the subsumptive-based engines of both YapTab and SLG-WAM. In the next tables,
we present the execution time of the RCS engine in milliseconds and the relative time
of the other engines by computing the value $T_{engine} / T_{RCS}$.
If the value is lesser than 1.0 then RCS performs worse, otherwise if the value is greater than
1.0 then RCS performs better. At the end of each table, we present the average values for
each engine.

Tables~\ref{tbl:overhead_detail_tst1}, \ref{tbl:overhead_detail_tst2} and \ref{tbl:overhead_detail_model}
shows the detailed results for the programs we use to measure the overhead.
Table~\ref{tbl:overhead_overview} shows the average values computed for each benchmark program
for call subsumption in the SLG-WAM and in the YapTab engine.
By analyzing the results we can see that YapTab+RCS performs worse
than YapTab with subsumptive-based tabling in most cases, only the
programs \texttt{right\_first} and the \texttt{right\_last} program show better results,
while the \texttt{left\_first} program has comparable execution times.
In theory, these benchmarks should not run faster, but cache effects and other
conditions could affect positively the execution time of these programs.

The average values show that
RCS is very competitive against XSB, because it is, in average, 21\% slower than RCS. More
importantly, when comparing RCS to YapTab with traditional call subsumption, RCS performs 5\% slower,
which shows that RCS adds a very small overhead when executing programs that do not benefit from the
new evaluation model.

\begin{table}[ht]
\centering
  \begin{tabular}{ccc}
   \hline
    \hline
    \multirow{2}{*}{\textbf{Program}} & \textbf{SLG-WAM} & \textbf{YapTab} \\
    & \textbf{\textit{\small{Sub / Retro}}} & \textbf{\textit{\small{Sub / Retro}}} \\
   \hline
   \hline
double\_first & 1.11 & 0.89 \\
double\_last & 1.03 & 0.87 \\
left\_first & 1.29 & \textbf{1.00} \\
left\_last &  1.26  & 0.92 \\
right\_first & 0.92 & \textbf{1.06} \\
right\_last & 1.00 & \textbf{1.08} \\
samegen & 0.78 & 0.91 \\
reach\_first  &  1.801  & 0.93 \\
reach\_last  &  1.703  & 0.92 \\
\hline
\hline
\textit{Average} &  1.21 &  0.95 \\
\hline
\hline
\end{tabular}
\caption{Average overhead values.}
\label{tbl:overhead_overview}
\end{table}

\input{tables_retro_overhead_model}
\input{tables_retro_overhead_tst}

\subsection{RCS Gains}

\subsection{STST Table Space Analysis}

\subsection{Overheads On Other Evaluation Modes}

